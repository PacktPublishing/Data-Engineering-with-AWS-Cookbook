# Data-Engineering-with-AWS-Cookbook
Data Engineering with AWS Cookbook, published by Packt.

Following the chapter list with the link to the companion files as needed.

## [Chapter 1](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter01)
- [Controlling access to S3 buckets](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter01/Recipe1)
- [Storage types in S3 for optimized storage costs](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter01/Recipe2)
- [Enforcing encryption of S3 buckets](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter01/Recipe3)
- Setting up retention policies for your objects
- Versioning your data
- Replicating your data
- Monitoring your S3 buckets

## [Chapter 2](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter02)
- Creating read-only replicas for RDS
- [Redshift live data sharing among your clusters](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter02/Recipe2)
- [Synchronizing Glue Data Catalog to a different account](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter02/Recipe3)
- Enforcing fine-grained permissions on S3 data sharing using Lake Formation
- [Sharing your S3 data temporarily using a presigned URL](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter02/Recipe5)
- [Real-time sharing of S3 data](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter02/Recipe6)
- Sharing read-only access to your CloudWatch data with another AWS account

## [Chapter 3](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03)
- [Creating ETL jobs visually using AWS Glue Studio](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe1)
- [Parameterizing jobs to make them more flexible and reusable](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe2)
- [Handling job failures and reruns for partial results](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe3)
- [Processing data incrementally using bookmarks and bounded execution](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe4)
- [Handling a high quantity of small files in your job](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe5)
- [Reusing libraries in your Glue job](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe6)
- [Using data lake formats to store your data](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe7)
- [Optimizing your catalog data retrieval using pushdown filters and indexes](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe8)
- [Running pandas code using AWS Glue for Ray](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter03/Recipe9)

## [Chapter 4](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04)
- [Defining a simple workflow using AWS Glue Workflows](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe1)
- [Setting up event-driven orchestration with AWS Event bridge](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe2)
- [Creating a data workflow using AWS Step Functions](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe3)
- [Managing Data Pipelines with MWAA](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe4)
- [Monitoring your pipeline health](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe5)
- [Setting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter04/Recipe6)

## [Chapter 5](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05)
- [Running jobs using AWS EMR serverless](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe1)
- [Running your AWS EMR cluster on EKS](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe2)
- [Using the AWS Glue catalog from another account](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe3)
- [Making your cluster highly available](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe4)
- [Scaling your cluster based on workload](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe5)
- [Customizing the cluster nodes easily using bootstrap actions](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe6)
- [Tuning Apache Spark resource usage](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe7)
- [Code development on EMR using Workspaces](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe8)
- [Monitoring your cluster](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe9)
- [Protecting your cluster from security vulnerabilities](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter05/Recipe10)

## [Chapter 6](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter06)
- [Applying Data Quality check on Glue tables](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter06/Recipe1)
- Automating the discovery and reporting of sensitive data on your S3 buckets
- [Establishing a tagging strategy for AWS resources](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter06/Recipe3)
- Building your distributed data community with AWS DataZone following data mesh principles
- [Handling security-sensitive data (PII and PHI)](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter06/Recipe5)
- [Ensuring S3 compliance with AWS Config](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter06/Recipe6)

## [Chapter 7](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter07)
- [Creating Data Quality for ETL jobs in AWS Glue Studio notebooks](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter07/Recipe1.ipynb)
- [Unit testing your data quality using Deequ](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter07/Recipe2)
- Schema management for ETL pipeline
- [Building unit test functions for ETL pipeline](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter07/Recipe4)
- Building data cleaning and profiling jobs with DataBrew

## [Chapter 8](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08)
- [Setting up a code deployment pipeline using CDK and AWS CodePipeline](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe1)
- [Setting up a CDK pipeline to deploy on multiple accounts and regions](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe2)
- [Running code in a CloudFormation deployment](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe3)
- [Protecting resources from accidental deletion](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe4)
- [Deploying a data pipeline using Terraform](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe5)
- [Reverse-engineering IaC](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe6)
- [Integrating AWS Glue and Git version control](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter08/Recipe7)

## [Chapter 9](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter09)
- [Automatically setting CloudWatch log group retention to reduce cost](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter09/Recipe1)
- Creating custom dashboards to monitor Data Lake services
- Setting up System Manager to remediate non-compliance with AWS Config rules
- Using AWS config to automate non-compliance S3 server access logging policy
- Tracking AWS Data Lake cost per analytics workload

## [Chapter 10](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter10)
- [Accessing the Redshift cluster using JDBC to query data](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe1)
- [Creating a VPC Endpoint to establish public connectivity between a private Redshift cluster and client applications](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe2)
- [Querying large historical data with Redshift Spectrum](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe3)
- [Using Redshift workload management to manage workload priority](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe4)
- [Using AWS SDK for Pandas, Redshift Data API and Lambda to execute SQL statements](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe5)
- [Using AWS SDK for Python to manage Amazon QuickSight](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter10/Recipe6)

## [Chapter 11](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter11)
- Reviewing the steps and processes for migrating an on-premises platform to AWS
- Choosing your AWS analytics stack – the re-platforming approach
- Picking the correct migration approach for your workload
- Planning for prototyping and testing
- Converting ETL processes with big data frameworks
- Defining and executing your migration process with Hadoop
- Migrating the existing Hadoop security authentication and authorization processes

## [Chapter 12](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter12)
- Creating SCT migration assessment report with AWS SCT
- Extracting Data with AWS DMS
- Live example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT
- [Leveraging AWS Snow Family for large-scale data migration](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter12/snowball-policy.json)

## [Chapter 13](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/tree/main/Chapter13)
- Calculating total cost of ownership (TCO) using AWS TCO calculators
- Conducting a Hadoop migration assessment using the TCO simulator
- Selecting how to store your data
- Migrating on-premises HDFS data using AWS DataSync
- [Migrating the Hive Metastore to AWS](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter13/GlueFederationJob%20.py)
- [Migrating and running Apache Oozie on Amazon EMR](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter13/bootstrap-script.sh)
- Migrating an Oozie database to the Amazon RDS MySQL
- [Setting up networking – establishing a secure connection to your EMR cluster
](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter13/emr-session-manager-policy.json)
- Performing a seamless HBase migration to AWS
- [Migrating HBase to DynamoDB on AWS](https://github.com/PacktPublishing/Data-Engineering-with-AWS-Cookbook/blob/main/Chapter13/dynamodb-import-glue-job.py)

