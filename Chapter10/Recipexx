from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame, DynamicFrameWriter
from awsglue.job import Job
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from pyspark.context import SparkContext
from pyspark.sql import *
import sys
import boto3

sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

# Define your Redshift credentials
datasink1 = glueContext.create_dynamic_frame.from_options(
    connection_type = "redshift",
    connection_options = {
        "url": "jdbc:redshift://fastdbwms-redshift-cluster.ca025yv2cvqa.ap-southeast-3.redshift.amazonaws.com:5439/fastdbwms", 
        "user": <<username for redshift>>, 
        "password": <<password for redshift>>,
        "dbtable": "public.test_table", 
        "redshiftTmpDir": "s3://aws-glue-assets-121e52980caf-ap-southeast-3/temporary/"
    }
)

insert_query = "insert into public.test_table select * from bdm.bdm_dev_cb_kinetica where trx_date='2023-01-01';"

datasink4 = glueContext.write_dynamic_frame.from_jdbc_conf(
	frame = datasink1, 
	catalog_connection = "glue-to-redshift", 
	connection_options = {
		#"preactions":"truncate table public.test_table",
		"dbtable": "public.test_table", 
		"database": "fastdbwms",
		"postactions":insert_query,
		"aws_iam_role": "arn:aws:iam::319259191038:role/redshift-role"
	},
	redshift_tmp_dir = "s3://aws-glue-assets-121e52980caf-ap-southeast-3/temporary/",
	transformation_ctx = "datasink4"
)

job.commit()
